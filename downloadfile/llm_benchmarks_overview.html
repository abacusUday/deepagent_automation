<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Evaluating Large Language Models: A Benchmark Overview</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/dist/reveal.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/dist/theme/serif.min.css">
  <style>
    :root {
      --standard-gap: 20px;
      --standard-padding: 20px;
      --card-bg-color: rgba(245, 245, 245, 0.9);
      --border-color: rgba(0, 0, 0, 0.1);
      --border-radius: 6px;
      --accent-color: #3498db;
      --text-color: #333;
      --heading-color: #2c3e50;
    }

    body {
      font-family: "Arial", "Helvetica", sans-serif;
      color: var(--text-color);
    }

    .reveal {
      font-size: 28px;
    }

    .slide-content {
      width: 1200px;
      height: 700px;
      display: grid;
      grid-template-rows: minmax(50px, auto) minmax(1fr, 80%) minmax(50px, auto);
      gap: var(--standard-gap);
    }

    .slide-header {
      padding: 0 var(--standard-padding);
    }

    .slide-body {
      overflow: hidden;
      padding: 0 var(--standard-padding);
    }

    .slide-footer {
      padding: 0 var(--standard-padding);
      font-size: 0.7em;
      color: rgba(0, 0, 0, 0.5);
    }

    .slide-number {
      left: 12px !important;
      right: unset !important;
      bottom: 12px !important;
      pointer-events: none !important;
    }

    /* Title slide specific */
    .title-slide h1 {
      margin-bottom: 0.3em;
      color: var(--heading-color);
    }

    .title-slide h3 {
      margin-top: 0;
      color: rgba(0, 0, 0, 0.7);
    }

    /* Grid layouts */
    .two-column {
      display: grid;
      grid-template-columns: 50% 50%;
      gap: var(--standard-gap);
      height: 100%;
    }

    .three-column {
      display: grid;
      grid-template-columns: 33% 33% 34%;
      gap: var(--standard-gap);
      height: 100%;
    }

    .sidebar-main {
      display: grid;
      grid-template-columns: 33% 67%;
      gap: var(--standard-gap);
      height: 100%;
    }

    .two-rows {
      display: grid;
      grid-template-rows: 50% 50%;
      gap: var(--standard-gap);
      height: 100%;
    }

    .three-rows {
      display: grid;
      grid-template-rows: 33% 33% 34%;
      gap: var(--standard-gap);
      height: 100%;
    }

    .topbar-main {
      display: grid;
      grid-template-rows: 33% 67%;
      gap: var(--standard-gap);
      height: 100%;
    }

    .main-bottom {
      display: grid;
      grid-template-rows: 67% 33%;
      gap: var(--standard-gap);
      height: 100%;
    }

    .four-quadrants {
      display: grid;
      grid-template-columns: 50% 50%;
      grid-template-rows: 50% 50%;
      gap: var(--standard-gap);
      height: 100%;
    }

    /* Content containers */
    .content-card {
      height: 100%;
      width: 100%;
      overflow: hidden;
      padding: var(--standard-padding);
      background: var(--card-bg-color);
      border-radius: var(--border-radius);
      box-sizing: border-box;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    }

    /* Table styling */
    .table-wrapper {
      height: 100%;
      width: 100%;
      overflow: hidden;
      padding: var(--standard-padding);
      background: var(--card-bg-color);
      border-radius: var(--border-radius);
      box-sizing: border-box;
    }

    .table-wrapper .autofit > table {
      border: 1px solid var(--border-color);
      border-collapse: collapse;
      table-layout: fixed;
      width: 100%;
    }

    .table-wrapper .autofit > table th,
    .table-wrapper .autofit > table td {
      border: 1px solid var(--border-color);
      word-wrap: break-word;
      padding: 0.4em 0.6em;
    }

    .table-wrapper .autofit > table th {
      background-color: rgba(0, 0, 0, 0.05);
      font-weight: bold;
    }

    .table-wrapper .autofit > table tr:nth-child(even) {
      background-color: rgba(0, 0, 0, 0.02);
    }

    /* Ensure all borders are visible */
    .table-wrapper .autofit > table tr:last-child td {
      border-bottom: 1px solid var(--border-color);
    }

    .table-wrapper .autofit > table td:last-child {
      border-right: 1px solid var(--border-color);
    }

    /* Lists and text elements */
    .content-card ul {
      margin-left: 1em;
      padding-left: 1em;
    }

    .content-card li {
      margin-bottom: 0.5em;
    }

    /* Image container */
    .image-container {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100%;
      overflow: hidden;
    }

    .image-container img {
      max-width: 100%;
      max-height: 100%;
      object-fit: contain;
    }

    /* Title slide background */
    .title-background {
      background-size: cover;
      background-position: center;
      position: relative;
    }

    .title-background::after {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: rgba(255, 255, 255, 0.7);
      z-index: -1;
    }

    /* Helpers */
    .text-center {
      text-align: center;
    }

    .mb-0 {
      margin-bottom: 0;
    }

    .mt-0 {
      margin-top: 0;
    }

    /* Custom slide styles */
    h1, h2, h3, h4 {
      color: var(--heading-color);
    }

    h3 {
      font-size: 1.5em;
      margin-bottom: 0.7em;
    }

    .chart-container {
      width: 100%;
      height: 100%;
      display: flex;
      justify-content: center;
      align-items: center;
    }
  </style>
</head>
<body>
  <div class="reveal">
    <div class="slides">
      <!-- Title Slide -->
      <section>
        <div class="slide-content">
          <div class="slide-header"></div>
          <div class="slide-body title-slide" style="background-image: url('https://cdn.abacus.ai/images/f5564731-4008-4fbf-b133-966774838714.png'); background-size: cover; background-position: center; position: relative;">
            <div style="position: absolute; top: 0; left: 0; right: 0; bottom: 0; background: rgba(255, 255, 255, 0.7); z-index: 0;"></div>
            <div class="text-center" style="position: relative; z-index: 1; padding-top: 10%;">
              <div class="autofit">
                <h1>EVALUATING LARGE LANGUAGE MODELS</h1>
                <h2>A BENCHMARK OVERVIEW</h2>
                <h3>Understanding How We Measure LLM Performance</h3>
                <p style="margin-top: 2em;">Presentation by [Your Name/Organization]</p>
              </div>
            </div>
          </div>
          <div class="slide-footer"></div>
        </div>
      </section>

      <!-- Slide 1: Why Do We Need LLM Benchmarks? -->
      <section>
        <div class="slide-content">
          <div class="slide-header">
            <h3>WHY DO WE NEED LLM BENCHMARKS?</h3>
          </div>
          <div class="slide-body two-column">
            <div class="content-card">
              <div class="autofit">
                <ul>
                  <li>Standardized evaluation across different models</li>
                  <li>Measure specific capabilities and limitations</li>
                  <li>Track progress in the field over time</li>
                  <li>Guide research and development priorities</li>
                  <li>Enable comparison between different approaches</li>
                </ul>
              </div>
            </div>
            <div class="content-card">
              <div class="image-container">
                <img src="https://cdn.abacus.ai/images/046b887d-ede6-4167-8c46-3a736349115e.png" alt="LLM evaluation cycle diagram">
              </div>
            </div>
          </div>
          <div class="slide-footer"></div>
        </div>
      </section>

      <!-- Slide 2: Comprehensive Benchmark Suites -->
      <section>
        <div class="slide-content">
          <div class="slide-header">
            <h3>COMPREHENSIVE BENCHMARK SUITES</h3>
          </div>
          <div class="slide-body main-bottom">
            <div class="three-column">
              <div class="content-card">
                <div class="autofit">
                  <h4>MMLU</h4>
                  <ul>
                    <li>57 subjects across multiple domains</li>
                    <li>Multiple-choice format</li>
                    <li>Tests breadth of knowledge</li>
                    <li>Current leader: GPT-4 (86.4%)</li>
                  </ul>
                </div>
              </div>
              <div class="content-card">
                <div class="autofit">
                  <h4>HELM</h4>
                  <ul>
                    <li>Holistic evaluation framework</li>
                    <li>Multi-metric measurement</li>
                    <li>Evaluates robustness, calibration, bias</li>
                    <li>Standardized scenarios</li>
                  </ul>
                </div>
              </div>
              <div class="content-card">
                <div class="autofit">
                  <h4>BIG-bench</h4>
                  <ul>
                    <li>200+ diverse tasks</li>
                    <li>JSON and Programmatic formats</li>
                    <li>BIG-bench Lite (BBL): 24-task subset</li>
                    <li>Measures emergent capabilities</li>
                  </ul>
                </div>
              </div>
            </div>
            <div class="content-card">
              <div class="image-container">
                <img src="https://cdn.abacus.ai/images/d0dd5d02-8478-4332-9350-4d4e2c0c6725.png" alt="Comparison chart showing performance of top LLMs across benchmark suites">
              </div>
            </div>
          </div>
          <div class="slide-footer"></div>
        </div>
      </section>

      <!-- Slide 3: Testing Knowledge & Factuality -->
      <section>
        <div class="slide-content">
          <div class="slide-header">
            <h3>TESTING KNOWLEDGE & FACTUALITY</h3>
          </div>
          <div class="slide-body two-rows">
            <div class="content-card">
              <div class="autofit">
                <ul>
                  <li>Assess factual recall and verification capabilities</li>
                  <li>Measure propensity to generate false information</li>
                  <li>Key benchmarks:
                    <ul>
                      <li>MMLU: Tests across 57 academic subjects</li>
                      <li>TruthfulQA: Measures avoidance of falsehoods</li>
                      <li>FEVER: Fact verification with evidence</li>
                      <li>NaturalQuestions: Real user queries</li>
                    </ul>
                  </li>
                </ul>
              </div>
            </div>
            <div class="content-card">
              <div class="image-container">
                <img src="https://cdn.abacus.ai/images/7b19f512-3e5b-4e8b-936a-5aa2f77f1edf.png" alt="Performance comparison of leading LLMs on TruthfulQA and MMLU benchmarks">
              </div>
            </div>
          </div>
          <div class="slide-footer"></div>
        </div>
      </section>

      <!-- Slide 4: Evaluating Reasoning Abilities -->
      <section>
        <div class="slide-content">
          <div class="slide-header">
            <h3>EVALUATING REASONING ABILITIES</h3>
          </div>
          <div class="slide-body two-column">
            <div class="content-card">
              <div class="autofit">
                <ul>
                  <li>Test logical, causal, and multi-step reasoning</li>
                  <li>Evaluate problem decomposition abilities</li>
                  <li>Key benchmarks:
                    <ul>
                      <li>GSM8K: Grade-school math word problems</li>
                      <li>ARC: Science questions requiring reasoning</li>
                      <li>BBH (Big-Bench Hard): Diverse reasoning tasks</li>
                      <li>GPQA: Advanced scientific reasoning</li>
                      <li>BoolQ: Yes/no questions requiring inference</li>
                    </ul>
                  </li>
                </ul>
              </div>
            </div>
            <div class="content-card">
              <div class="image-container">
                <img src="https://debugml.github.io/assets/images/fcot/GSM8K_example.png" alt="Example of a multi-step reasoning problem from GSM8K with solution steps highlighted">
              </div>
            </div>
          </div>
          <div class="slide-footer"></div>
        </div>
      </section>

      <!-- Slide 5: Measuring Coding Capabilities -->
      <section>
        <div class="slide-content">
          <div class="slide-header">
            <h3>MEASURING CODING CAPABILITIES</h3>
          </div>
          <div class="slide-body two-rows">
            <div class="content-card">
              <div class="autofit">
                <ul>
                  <li>Assess ability to generate, understand, and debug code</li>
                  <li>Evaluate algorithmic thinking</li>
                  <li>Key benchmarks:
                    <ul>
                      <li>HumanEval: Functionality via unit tests</li>
                      <li>CodeXGLUE: Multi-task code understanding</li>
                      <li>SWE-Bench: Real-world software engineering</li>
                      <li>LiveCodeBench: Competitive programming</li>
                      <li>MBPP: Basic Python programming tasks</li>
                    </ul>
                  </li>
                </ul>
              </div>
            </div>
            <div class="content-card">
              <div class="image-container">
                <img src="https://production-media.paperswithcode.com/datasets/a2af6cf1-212b-4a05-8d5c-55170a21ce05.png" alt="Code snippet example from HumanEval with annotations showing what aspects are being evaluated">
              </div>
            </div>
          </div>
          <div class="slide-footer"></div>
        </div>
      </section>

      <!-- Slide 6: Safety & Ethical Alignment -->
      <section>
        <div class="slide-content">
          <div class="slide-header">
            <h3>SAFETY & ETHICAL ALIGNMENT</h3>
          </div>
          <div class="slide-body two-column">
            <div class="content-card">
              <div class="autofit">
                <ul>
                  <li>Test for safety, toxicity, and alignment</li>
                  <li>Evaluate resilience against misuse</li>
                  <li>Key benchmarks:
                    <ul>
                      <li>AdvBench: Resistance to jailbreaking</li>
                      <li>RealToxicityPrompts: Offensive content</li>
                      <li>ETHICS: Moral reasoning capabilities</li>
                      <li>TruthfulQA: Honesty in challenging scenarios</li>
                      <li>Anthropic's RLHF evaluations</li>
                    </ul>
                  </li>
                </ul>
              </div>
            </div>
            <div class="content-card">
              <div class="image-container">
                <img src="https://cdn.abacus.ai/images/8748c931-f51e-425a-af37-bee163740355.png" alt="Diagram showing spectrum of model responses from safe/aligned to potentially harmful/misaligned">
              </div>
            </div>
          </div>
          <div class="slide-footer"></div>
        </div>
      </section>

      <!-- Slide 7: How Are Benchmarks Scored? -->
      <section>
        <div class="slide-content">
          <div class="slide-header">
            <h3>HOW ARE BENCHMARKS SCORED?</h3>
          </div>
          <div class="slide-body">
            <div class="four-quadrants">
              <div class="content-card">
                <div class="autofit">
                  <h4>Accuracy Metrics</h4>
                  <ul>
                    <li>Exact Match, Pass@k</li>
                    <li>F1/BLEU/ROUGE for generation</li>
                  </ul>
                </div>
              </div>
              <div class="content-card">
                <div class="autofit">
                  <h4>Probabilistic Metrics</h4>
                  <ul>
                    <li>Log probability</li>
                    <li>Calibration measures</li>
                  </ul>
                </div>
              </div>
              <div class="content-card">
                <div class="autofit">
                  <h4>Multi-Metric Evaluation</h4>
                  <ul>
                    <li>HELM's multi-dimensional approach</li>
                    <li>Composite scoring systems</li>
                  </ul>
                </div>
              </div>
              <div class="content-card">
                <div class="autofit">
                  <h4>Human & LLM Evaluation</h4>
                  <ul>
                    <li>Expert ratings</li>
                    <li>LLM-as-Judge approaches</li>
                    <li>Preference-based comparisons</li>
                  </ul>
                </div>
              </div>
            </div>
          </div>
          <div class="slide-footer" style="display: flex; justify-content: center;">
            <div class="image-container" style="max-height: 100%;">
              <img src="https://cdn.abacus.ai/images/a2b0444d-ecd0-4324-b4db-7954268eaa61.png" alt="Circular diagram showing how different evaluation methods complement each other" style="max-height: 45px;">
            </div>
          </div>
        </div>
      </section>

      <!-- Slide 8: Benchmark Limitations -->
      <section>
        <div class="slide-content">
          <div class="slide-header">
            <h3>BENCHMARK LIMITATIONS</h3>
          </div>
          <div class="slide-body two-column">
            <div class="content-card">
              <div class="autofit">
                <ul>
                  <li>Benchmark saturation as models approach human performance</li>
                  <li>Data contamination concerns</li>
                  <li>Implementation sensitivity (prompt format, scoring)</li>
                  <li>Gap between benchmark performance and real-world utility</li>
                  <li>Narrow focus of individual benchmarks</li>
                  <li>Rapid evolution requiring constant updates</li>
                  <li>Challenges in evaluating emerging capabilities</li>
                </ul>
              </div>
            </div>
            <div class="content-card">
              <div class="image-container">
                <img src="https://cdn.abacus.ai/images/899c8d0f-50fb-458a-84f6-a1cde57bb483.png" alt="Graph showing benchmark saturation over time as models approach human performance">
              </div>
            </div>
          </div>
          <div class="slide-footer"></div>
        </div>
      </section>

      <!-- Slide 9: The Future of LLM Evaluation -->
      <section>
        <div class="slide-content">
          <div class="slide-header">
            <h3>THE FUTURE OF LLM EVALUATION</h3>
          </div>
          <div class="slide-body two-column">
            <div class="content-card">
              <div class="autofit">
                <ul>
                  <li>Emerging trends:
                    <ul>
                      <li>Multimodal evaluation incorporating text, images, audio</li>
                      <li>Adversarial testing with increasingly difficult challenges</li>
                      <li>Domain-specific benchmarks for specialized applications</li>
                      <li>Continuous benchmarking during development</li>
                      <li>Standardized frameworks (HELM, EleutherAI Harness)</li>
                      <li>Regulatory influence on evaluation standards</li>
                    </ul>
                  </li>
                  <li>The quest continues for more comprehensive, challenging, and relevant benchmarks</li>
                </ul>
              </div>
            </div>
            <div class="content-card">
              <div class="image-container">
                <img src="https://cdn.abacus.ai/images/39df6cf4-f807-48d7-b483-fdc72a7daa3e.png" alt="Futuristic image representing next-generation AI evaluation methods">
              </div>
            </div>
          </div>
          <div class="slide-footer"></div>
        </div>
      </section>
    </div>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/dist/reveal.js"></script>
  <script>
    // RevealJS initialization
    Reveal.initialize({
      width: 1200,
      height: 700,
      center: false,
      slideNumber: 'c/t',
      controls: true,
      progress: true,
      margin: 0.07,
      minScale: 0.2,
      maxScale: 1.5,
    });
  </script>
  <script>
    function isOverflown(el) {
      return el.scrollHeight > el.clientHeight + 1;
    }

    // Font size adjustment function
    function shrinkTextIfNeeded(el, options = {}) {
      const minFont = options.minFontSize || 8;
      let originalFont = parseFloat(window.getComputedStyle(el).fontSize) || 16;

      if (!isOverflown(el.parentElement) && !isOverflown(el)) {
        return;
      }

      let left = minFont;
      let right = originalFont;
      let bestFit = right;

      while (left <= right) {
        const mid = Math.floor((left + right) / 2);
        el.style.fontSize = mid + 'px';

        if (isOverflown(el.parentElement) || isOverflown(el)) {
          right = mid - 1;
        } else {
          bestFit = mid;
          left = mid + 1;
        }
      }
      el.style.fontSize = bestFit + 'px';
    }

    function applyCustomShrink() {
      const tableWrappers = document.querySelectorAll('.table-wrapper .autofit');
      tableWrappers.forEach(el => {
        shrinkTextIfNeeded(el);
      });

      const autofitEls = document.querySelectorAll('.autofit');
      autofitEls.forEach(el => {
        if (!el.closest('.table-wrapper')) {
          shrinkTextIfNeeded(el);
        }
      });
    }

    Reveal.on('ready', function () {
      setTimeout(applyCustomShrink, 500);
    });
    Reveal.on('slidechanged', function () {
      setTimeout(applyCustomShrink, 500);
    });
    Reveal.on('resize', function () {
      setTimeout(applyCustomShrink, 500);
    });
  </script>
</body>
</html>